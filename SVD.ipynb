{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (2 * 13, 2 * 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a796a150b242838ef6dec36f12c62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "uids = []\n",
    "targets = []\n",
    "texts = []\n",
    "hotel_ids = []\n",
    "\n",
    "words = Counter()\n",
    "with open('tokenize_reviewContent', 'r') as f:\n",
    "    for line in tqdm_notebook(f):\n",
    "        line = line.strip()\n",
    "        uid, hotel_id, year, mark, target, text = line.split('\\t')\n",
    "        \n",
    "        targets.append(int(target))\n",
    "        uids.append(int(uid))\n",
    "        hotel_ids.append(int(hotel_id))\n",
    "\n",
    "        res = []\n",
    "        for word in text.split():\n",
    "            if word not in english_stopwords and len(word) > 2:\n",
    "                flag = True\n",
    "\n",
    "                for alpha in word:\n",
    "                    if not alpha.isalpha():\n",
    "                        flag = False\n",
    "                        break\n",
    "                \n",
    "                if flag:\n",
    "                    res.append(word)\n",
    "                    words[word] += 1\n",
    "\n",
    "        texts.append(' '.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_words(words_count, min_count = 10):\n",
    "    tokens = Counter()\n",
    "    for token, val in words_count.items():\n",
    "        if val >= min_count:\n",
    "            tokens[token] += val\n",
    "    return tokens\n",
    "\n",
    "def word_to_vocab(words):\n",
    "    vocab = dict()\n",
    "    for index, word in enumerate(sorted(words.keys())):\n",
    "        vocab[word] = index\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_to_vocab(del_words(words, min_count = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(vocabulary = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.9 s, sys: 420 ms, total: 22.3 s\n",
      "Wall time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TruncatedSVD(n_components = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 54s, sys: 1min 14s, total: 23min 8s\n",
      "Wall time: 10min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_svd = model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('hotels_svd_1000', X_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49119c217133425fa753ab26073b4c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dim in tqdm_notebook([3, 10, 30, 50, 100, 300, 1000]):\n",
    "    X_small = X_svd[:, :dim]\n",
    "    kdt = KDTree(X_small, metric='euclidean')\n",
    "    kdt.query(X_small, k = 51, return_distance = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hotels_svd_text.pickle', 'wb') as f:\n",
    "    pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "sent_tokenizer = nltk.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f401174e63341ae9dd9aa0c47daf596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('sent_tokenize_reviewContent', 'w') as out_1:\n",
    "    with open('tokenize_reviewContent', 'w') as out_2:\n",
    "        with open('./data/metadata') as f1:\n",
    "            with open('./data/reviewContent') as f2:\n",
    "                for line1, line2 in tqdm_notebook(zip(f1, f2)):\n",
    "                    _, _, mark, target, _ = line1.strip().split()\n",
    "                    target = int(target)\n",
    "                    if target == 1:\n",
    "                        target = 0\n",
    "                    else:\n",
    "                        target = 1\n",
    "                    target = str(target)\n",
    "                    \n",
    "                    uid, hotel_id, year, text = line2.strip().split('\\t', 3)\n",
    "\n",
    "                    for sent in sent_tokenizer(text):\n",
    "                        sent = ' '.join(word_tokenizer.tokenize(sent))\n",
    "                        out_1.write('\\t'.join([uid, hotel_id, year, mark, target, sent]))\n",
    "                        out_1.write('\\n')\n",
    "\n",
    "                    text = ' '.join(word_tokenizer.tokenize(text))\n",
    "                    \n",
    "                    out_2.write('\\t'.join([uid, hotel_id, year, mark, target, text]))\n",
    "                    out_2.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "\n",
    "with open('./data/reviewContent') as f:\n",
    "    for line in tqdm_notebook(f):\n",
    "        line = line.strip()\n",
    "        uid, hotel_id, year, text = line.split('\\t', 3)\n",
    "        for word in sent_tokenizer(text):\n",
    "            if word not in english_stopwords:\n",
    "                flag = True\n",
    "                for alpha in word:\n",
    "                    if not alpha.isalpha():\n",
    "                        flag = False\n",
    "                        break\n",
    "                if flag:\n",
    "                    words_count[word] += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "\n",
    "words_count = Counter()\n",
    "\n",
    "with open('./data/reviewContent') as f:\n",
    "    for line in tqdm_notebook(f):\n",
    "        line = line.strip()\n",
    "        uid, hotel_id, year, text = line.split('\\t', 3)\n",
    "        text = tokenizer.tokenize(text.lower())\n",
    "        for word in text:\n",
    "            if word not in english_stopwords:\n",
    "                flag = True\n",
    "                for alpha in word:\n",
    "                    if not alpha.isalpha():\n",
    "                        flag = False\n",
    "                        break\n",
    "                if flag:\n",
    "                    words_count[word] += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(words_count.values()), range=[0, 200000], bins=50, log=True)\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finall_words = del_words(words_count, min_count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(finall_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(finall_words.values()), range=[0, 200000], bins=50, log=True)\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = set()\n",
    "with open('stem.txt') as f:\n",
    "    index = 0\n",
    "    for line in tqdm_notebook(f):\n",
    "        line = line.strip()\n",
    "        if len(line.split('\\t')) < 4:\n",
    "            continue\n",
    "        uid, hotel, date, text = line.split('\\t')\n",
    "        \n",
    "        for word in text.split():\n",
    "            for alpha in word:\n",
    "                if not alpha.isalpha():\n",
    "                    bad_words.add(word)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = []\n",
    "hotels = []\n",
    "dates = []\n",
    "texts = []\n",
    "\n",
    "hotels_2_index = defaultdict(list)\n",
    "uids_2_index = defaultdict(list)\n",
    "with open('stem.txt') as f:\n",
    "    index = 0\n",
    "    for line in tqdm_notebook(f):\n",
    "        line = line.strip()\n",
    "        if len(line.split('\\t')) < 4:\n",
    "            continue\n",
    "        uid, hotel, date, text = line.split('\\t')\n",
    "        \n",
    "        text = [word for word in text.split() if word not in bad_words]\n",
    "        text = ' '.join(text)\n",
    "        \n",
    "        hotels_2_index[int(hotel)].append(index)\n",
    "        uids_2_index[int(uid)].append(index)\n",
    "        index += 1\n",
    "        \n",
    "        hotels.append(int(hotel))\n",
    "        uids.append(int(uid))\n",
    "        dates.append(date)\n",
    "        texts.append(text)\n",
    "\n",
    "targets = [0] * len(texts)\n",
    "\n",
    "with open('./data/metadata') as f:\n",
    "    for line in f:\n",
    "        uid, hotel, mark, label, date = line.strip().split()\n",
    "        label = int(label)\n",
    "        uid = int(uid)\n",
    "        if uid in uids_2_index:\n",
    "            for index in uids_2_index[uid]:\n",
    "                targets[index] = label\n",
    "        else:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df = 0.95)\n",
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..X_small = svd.fit_transform(X)\n",
    "y = np.array(targets)\n",
    "y += 1\n",
    "y = np.divide(y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..y = np.array(targets)\n",
    "y *= -1\n",
    "y += 1\n",
    "y = np.divide(y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_small, y, test_size = 0.25, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proba = model.predict_proba(X_train)[:, 1]\n",
    "train_predict = model.predict(X_train)\n",
    "print('ROC-AUC = {}, F-meausre = {}, RECALL = {}, PRECISION = {}'.format(\\\n",
    "                                                                        roc_auc_score(y_train, train_proba),\\\n",
    "                                                                        f1_score(y_train, train_predict),\\\n",
    "                                                                        recall_score(y_train, train_predict),\\\n",
    "                                                                        precision_score(y_train, train_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "choosen_hotel = None\n",
    "for hotel, lst in hotels_2_index.items():\n",
    "    if len(lst) > max_len:\n",
    "        print(hotel, len(lst))\n",
    "        max_len = len(lst)\n",
    "        choosen_hotel = hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_choosen = X_small[hotels_2_index[choosen_hotel]]\n",
    "y_choosen = y[hotels_2_index[choosen_hotel]]\n",
    "\n",
    "X_choosen = (X_choosen.T / np.linalg.norm(X_choosen, axis = 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_choosen[y_choosen == 0]), len(y_choosen[y_choosen == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters = 10, max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predict = model.fit_predict(X_choosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_to_fit = X_choosen[:, 5:]\n",
    "for k in [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]:\n",
    "    model = KMeans(n_clusters = k, max_iter = 1000)\n",
    "    predict = model.fit_predict(X_to_fit)\n",
    "    clusters = [[0, 0] for i in range(np.max(predict) - np.min(predict) + 1)]\n",
    "    for i, cluster in enumerate(predict):\n",
    "        clusters[cluster][targets[i]] += 1\n",
    "    print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_choosen, y_choosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score\n",
    "roc_auc_score(y_choosen, model.predict_proba(X_choosen)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_choosen, model.predict(X_choosen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, target in zip(texts, targets):\n",
    "    if target == -1:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./data/metadata') as f:\n",
    "    for line in f:\n",
    "        uid, hotel, mark, label, date = line.strip().split()\n",
    "        label = int(label)\n",
    "        uid = int(uid)\n",
    "        if uid in uids_2_index:\n",
    "            if label == -1:\n",
    "                for index in uids_2_index[uid]:\n",
    "                    print(texts[index])\n",
    "                print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look on original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "texts = []\n",
    "uids_2_index = defaultdict(list)\n",
    "with open('./data/reviewContent') as f:\n",
    "    for index, line in tqdm_notebook(enumerate(f)):\n",
    "        uid, hotel, date, text = line.strip().split('\\t', 3)\n",
    "        texts.append(text)\n",
    "        uids_2_index[int(uid)].append(index)\n",
    "\n",
    "targets = [0] * len(texts)\n",
    "\n",
    "with open('./data/metadata') as f:\n",
    "    for index, line in tqdm_notebook(enumerate(f)):\n",
    "        uid, hotel, mark, label, date = line.strip().split()\n",
    "        label = int(label)\n",
    "        uid = int(uid)\n",
    "        targets[index] = label\n",
    "        if uid not in uids_2_index:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_count = 0\n",
    "zero_pos_count = 0\n",
    "one_count = 0\n",
    "for uid in uids_2_index:\n",
    "    lst  = uids_2_index[uid]\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for index in lst:\n",
    "        if targets[index] == -1:\n",
    "            neg += 1\n",
    "        else:\n",
    "            pos += 1\n",
    "\n",
    "    if neg != 0:\n",
    "        all_count += 1\n",
    "        if pos == 0:\n",
    "            zero_pos_count += 1\n",
    "            if neg == 1:\n",
    "                one_count += 1\n",
    "                for index in lst:\n",
    "                    print('\\t' + texts[index])\n",
    "                print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_count, zero_pos_count, one_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
